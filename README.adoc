= TF-Roach
:linkattrs:
:project-owner: amineelkouhen
:project-name:  tf-roach
:project-group: com.cockroachlabs
:project-version:   1.0.0
:site-url:  https://github.com/amineelkouhen/tf-roach

image:https://img.shields.io/github/contributors/{project-owner}/{project-name}[GitHub contributors]
image:https://img.shields.io/github/forks/{project-owner}/{project-name}[Fork]
image:https://img.shields.io/github/stars/{project-owner}/{project-name}[GitHub Repo stars]
image:https://img.shields.io/github/watchers/{project-owner}/{project-name}[GitHub watchers]
image:https://img.shields.io/github/issues/{project-owner}/{project-name}[GitHub issues]
image:https://img.shields.io/github/license/{project-owner}/{project-name}[License]

image::images/tf-roach.jpg[CRL]
---

TF-Roach is a set of Terraform templates designed to provision different kinds of CockroachDB Clusters across multiple cloud vendors.

Currently only AWS, GCP and Azure are supported.

The terraform state file is currently maintained locally. This means:

* Only one deployment is supported for each directory where the script is executed (terraform state file)
* Deployments created by other individuals will not be updatable

== Prerequisites

- Install https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^]
- Create an SSH key file (~/.ssh/id_rsa)


=== AWS setup

- Download an https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[AWS service account key file^]
- In the file `variables.tf`, update the variables "aws_access_key" and "aws_secret_key" with the first and second entries of the AWS service account key file.

== CockroachDB - Architecture

CockroachDB is a distributed SQL database built on a transactional and strongly-consistent key-value store. It scales horizontally; survives disk, machine, rack, and even datacenter failures with minimal latency disruption and no manual intervention; supports strongly-consistent ACID transactions; and provides a familiar SQL API for structuring, manipulating, and querying data.

CockroachDB scales horizontally with minimal operator overhead: At the key-value level, CockroachDB starts off with a single, empty range. As you put data in, this single range eventually reaches a threshold size. When that happens, the data splits into two ranges, each covering a contiguous segment of the entire key-value space. This process continues indefinitely; as new data flows in, existing ranges continue to split into new ranges, aiming to keep a relatively small and consistent range size.

image::images/ranges.gif[CRL]

A CockroachDB cluster is composed of identical nodes that are deployed within a data center or stretched across multiple regions / availability zones.

image::images/multi-regional.png[CRL]

== CockroachDB - Deployment

In the `main` folder, there exist multiple configurations that create CockroachDB Clusters, on the main cloud providers, and with multiple deployment options:

- on Virtual Machines (Self-hosted),
- Managed (using CockroachDB serverless) => TBC,
- on Kubernetes (ex. GKE) => TBC

=== CockroachDB on Virtual Machines

[cols="25h,~,~,~"]
|===
.2+^.^h|Provider .2+^.^h|Configuration 2.+^.^h|Cluster Type 
^.^h|Basic Cluster ^.^h|Multi-Zonal Cluster (HA)

.2+^.^|Amazon Web Services (AWS)

^.^|Regional Deployment
a|image::images/TBC.png[Basic Cluster, link=""]
a|image::images/TBC.png[Multi-Zonal, link=""]

^.^|Multi-Region Deployment
a|image::images/TBC.png[Basic Cluster, link=""]
a|image::main/AWS/Multi-Region/Multi_Zonal_Clusters/images/Multi-Region.svg[Multi-Zonal, link="main/AWS/Multi-Region/Multi_Zonal_Clusters"]

|===

Each configuration consists of one (or many) JSON file(s) (*.tf.json) that calls one or many modules depending on the configuration.
For each cloud provider, it exists a networking module to create VPCs/VNETs, subnets and load balancers, a DNS module to create the cluster's FQDN, and a cockroach (cr) module that creates the cluster nodes.
Other modules exist for specific purposes like the peering or keypair modules...

If a client is added (the bastion module is called), a standalone machine will be created with a few pre-installed packages (db-workload, the trade demo application, and much more).

image::images/trade_demo.png[CRL]

Example of a configuration file

[source,json]
{
    "provider": {
        "aws": {
            "region": "${var.region_name}",
            "access_key": "${var.aws_access_key}",
            "secret_key": "${var.aws_secret_key}"
        }
    },
    "module": {
        "network-vpc": {
            "source": "../../../../modules/aws/network",
            "name": "${var.deployment_name}-${var.env}",
            "vpc_cidr": "${var.vpc_cidr}",
            "resource_tags" : {},
            "subnets_cidrs": "${var.subnets}"
        },
        "keypair": {
            "source": "../../../../modules/aws/keypair",
            "name": "${var.deployment_name}-${var.env}",
            "ssh_public_key": "${var.ssh_public_key}",
            "resource_tags" : {}
        },
        "cr-cluster": {
            "source": "../../../../modules/aws/cr",
            "name": "${var.deployment_name}-${var.env}",
            "worker_count": "${var.cluster_size}",
            "machine_type": "${var.machine_type}",
            "machine_image": "${var.machine_images}",
            "resource_tags" : {},
            "ssh_user": "${var.ssh_user}",
            "ssh_public_key": "${var.ssh_public_key}",
            "ssh_key_name": "${module.keypair.key-name}",
            "security_groups": "${module.network-vpc.security-groups}",
            "region": "${var.region}",
            "availability_zones": "${keys(var.subnets)}",
            "subnets": "${module.network-vpc.subnets}",
            "cockroach_release" : "${var.crdb_release}",
            "cluster_join_ips" : "${module.cr-cluster.cr-private-ips}",
            "boot_disk_size" : "${var.volume_size}",
            "boot_disk_type" : "${var.volume_type}"
        }
    }
}

==== Setup

- `terraform init`: To load all needed modules for the provisioning
- Review `variables.tf` to update variables like the project_name, the credentials (access and secret keys), the ssh_key... but also the configuration options like the number of nodes, type of machines, volume size, OS images, the regions, the cidr, the availability zones...

Example of the variables file

[source,hcl]
----
variable "region_name" {
  default = "us-east-1"
}

variable "vpc_cidr" {
  default = "10.1.0.0/16"
}

variable "subnets" {
  type = map
  default = {
    us-east-1a = "10.1.1.0/24"
  }
}

variable "ssh_public_key" {
  default = "~/.ssh/id_rsa.pub"
}

variable "ssh_user" {
  default = "ubuntu"
}

variable "cluster_size" {
  default = 3
}

variable "crdb_release" {
  default = "https://binaries.cockroachdb.com/cockroach-v24.1.1.linux-amd64.tgz"
}

variable "machine_type" {
  default = "t2.2xlarge"
}

variable "machine_image" {
  // Ubuntu 24.04 LTS
  default = "ami-04b70fa74e45c3917"
}
----

==== Usage


1. To check the terraform build plan, run the following command.

        terraform plan

2. To build the infrastructure, run the following command.

        terraform apply

The latest will setup a logical cluster (three physical clusters in case of a Multi-Region deployment), with 3 nodes, a VPC, subnet(s), route table(s), internet gateway(s) and a Network Load Balancer for each physical cluster (each region).

Example of a basic cluster output

....
Outputs:
####################################### Client #######################################

client-public-IP = "52.40.254.77"
backend-url = "http://52.40.254.77:3000/api/data"
frontend-url = "http://52.40.254.77:8080/"

console-url = "http://amine.cluster.sko-iam-demo.com:8080/"
connexion-string = "postgresql://root@amine.cluster.sko-iam-demo.com:26257/defaultdb"
####################################### CRDB Cluster #################################

cluster-dns = "amine.cluster.sko-iam-demo.com"
cr-cluster-nlb-dns = "nlb-20240703082428087000000004-b0e5c5f65a5943eb.elb.us-east-1.amazonaws.com"

cr-cluster-private-ips = [
  "10.1.1.75",
  "10.1.2.176",
  "10.1.3.188",
]
cr-cluster-public-ips = [
  "3.83.179.51",
  "3.88.140.128",
  "3.89.182.0",
]
....